{
 "cells": [
  {
   "cell_type": "code",
   "id": "6a0877db92b80b40",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import utils as ut\n",
    "\n",
    "organism = 'others'\n",
    "\n",
    "train_paths = [f'data/sp_data/train_set_partition_0_{organism}.json',\n",
    "               f'data/sp_data/train_set_partition_1_{organism}.json']\n",
    "val_paths = [f'data/sp_data/test_set_partition_0_{organism}.json',\n",
    "             f'data/sp_data/test_set_partition_1_{organism}.json']\n",
    "test_paths = [f'data/sp_data/train_set_partition_2_{organism}.json',\n",
    "              f'data/sp_data/test_set_partition_2_{organism}.json']\n",
    "\n",
    "train = pd.concat(pd.read_json(path) for path in ut.abspaths(train_paths))\n",
    "val = pd.concat(pd.read_json(path) for path in ut.abspaths(val_paths))\n",
    "test = pd.concat(pd.read_json(path) for path in ut.abspaths(test_paths))\n",
    "\n",
    "keys = train['kingdom'].unique()\n",
    "t = train['kingdom'].value_counts(sort=False).tolist()\n",
    "# print(t.iloc[0])\n",
    "\n",
    "plt.bar(keys, t, align='center')\n",
    "xlocs, xlabs = plt.xticks()\n",
    "for i, v in enumerate(t):\n",
    "    plt.text(xlocs[i] - 0.1, v + 100, str(v))\n",
    "plt.show()\n",
    "# plt.savefig(fname=str(Path(params)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c6814adc5216ddfa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import torch\n",
    "\n",
    "CKPT_PATH = './checkpoints/transformer_epoch=3_kaggle_v2.ckpt'\n",
    "\n",
    "checkpoint = torch.load(CKPT_PATH)\n",
    "print(checkpoint.keys())\n",
    "print(checkpoint['state_dict'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "train_paths = [f'data/sp_data/train_set_partition_0.json', f'data/sp_data/train_set_partition_1.json']\n",
    "val_paths = [f'data/sp_data/test_set_partition_0.json', f'data/sp_data/test_set_partition_1.json']\n",
    "test_paths = [f'data/sp_data/train_set_partition_2.json', f'data/sp_data/test_set_partition_2.json']\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "for path in train_paths:\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        train_data.extend(data)\n",
    "\n",
    "train_data = pd.DataFrame(train_data)\n",
    "train_data"
   ],
   "id": "281bc667de9be47c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "t = {\n",
    "    \"c\": 4,\n",
    "    \"b\": 2,\n",
    "    \"a\": 3,\n",
    "}\n",
    "\n",
    "c = dict(a=1, b=2, c=3)\n",
    "\n",
    "tt = dict(sorted(t.items(), key=lambda kv: c[kv[0]]))"
   ],
   "id": "97346703c1ca4ea2",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "list(tt.keys())",
   "id": "70c0828f21342775",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T08:54:43.650173Z",
     "start_time": "2024-05-30T08:54:43.639664Z"
    }
   },
   "cell_type": "code",
   "source": "len(\"MAPTLFQKLFSKRTGLGAPGRDARDPDCGFSWPLPEFDPSQIRLIVYQDCERRGRNVLFDSSVKRRNEDI\")",
   "id": "44e55b2b6b413100",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T09:02:23.896301Z",
     "start_time": "2024-05-30T09:02:23.858391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tokenizer.tokenizer_utils as tut\n",
    "import torch\n",
    "tokenizer = tut.load_tokenizer('cnn', 'smiles')\n",
    "encoded = tokenizer.batch_encode_plus([\"N[C@@H](CCSC)C(=O)N[C@@H](CCCCN)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCCCN)C(=O)N[C@@H](CC1=CC=C(O)C=C1)C(=O)N[C@@H](CC1=CC=CC=C1)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N[C@@H](CC1=CNC=N1)C(=O)N1[C@@H](CCC1)C(=O)N[C@@H](CO)C(=O)NCC(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC(C)C)C(=O)N1[C@@H](CCC1)C(=O)N[C@@H](CCCNC(N)=N)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H]([C@@H](C)O)C(=O)N[C@@H](CCSC)C(=O)NCC(=O)N[C@@H](C)C(=O)N[C@@H](CC(C))C(=O)N[C@@H](C)C(=O)N[C@@H](CS)C(=O)N[C@@H]([C@@H](C)O)C(=O)N[C@@H](CCCCN)C(=O)N[C@@H](CCC(O)=O)C(=O)N[C@@H](CC(O)=O)C(=O)N[C@@H](CC(N)=O)C(=O)N1[C@@H](CCC1)C(=O)N[C@@H](CC(O)=O)C(=O)N[C@@H](CCC(N)=O)C(=O)N1[C@@H](CCC1)C(=O)N[C@@H]([C@@H](C)O)C(=O)N[C@@H](CO)C(=O)N[C@@H](CC(O)=O)C(=O)N[C@@H](CCC(O)=O)C(=O)N[C@@H](CC(C))C(=O)N[C@@H](C)C(=O)N[C@@H]([C@@H](C)O)C(=O)N[C@@H](CC(C))C(=O)N[C@@H](CCCCN)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CO)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC(O)=O)C(=O)N[C@@H](CC(O)=O)C(=O)N[C@@H](CC(C))C(=O)N[C@@H](CCC(O)=O)C(=O)N[C@@H](CCSC)C(=O)N[C@@H](CCCNC(N)=N)C(=O)NCC(=O)NCC(=O)N[C@@H](CC(O)=O)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H](CC1=CC=C(O)C=C1)C(=O)N[C@@H](CO)C(=O)NCC(=O)N[C@@H](CCC(O)=O)C(=O)N[C@@H](CC(O)=O)C(=O)N[C@@H](CC(C)C)C(=O)N[C@@H]([C@@H](C)CC)C(=O)N[C@@H](CCCCN)C(=O)N[C@@H](CCCCN)C(=O)N[C@@H](CC(C))C(=O)O\"])\n",
    "print(torch.tensor(encoded['input_ids'][0]))"
   ],
   "id": "2c45e85864747cc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1, 13, 16, 11, 19, 12, 20, 38,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12,\n",
      "        20, 33,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 38,  6, 11, 21, 14,\n",
      "         6, 13, 16, 11, 19, 12, 20, 33,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12,\n",
      "        20, 25,  9, 18,  9, 11,  5, 14,  6, 11,  9, 27,  6, 11, 21, 14,  6, 13,\n",
      "        16, 11, 19, 12, 20, 25,  9, 18,  9, 18,  9, 27,  6, 11, 21, 14,  6, 13,\n",
      "        16, 11, 19, 12, 20, 25,  9, 36,  9, 29,  6, 11, 21, 14,  6, 13, 16, 11,\n",
      "        19, 12, 20, 25,  9, 36,  9, 29,  6, 11, 21, 14,  6, 29, 16, 11, 19, 12,\n",
      "        20, 35,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 31,  6, 11, 21, 14,\n",
      "         6, 28, 21, 14,  6, 13, 16, 11, 19, 12, 20, 18,  5, 11,  6, 11,  6, 11,\n",
      "        21, 14,  6, 13, 16, 11, 19, 12, 20, 18,  5, 11,  6, 11,  6, 11, 21, 14,\n",
      "         6, 29, 16, 11, 19, 12, 20, 35,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12,\n",
      "        20, 34,  5, 13, 22, 13,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 18,\n",
      "         5, 11,  6, 11,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 18,  5, 11,\n",
      "         6, 11,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 18,  5, 11,  6, 11,\n",
      "         6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 18,  5, 11,  6, 11,  6, 11,\n",
      "        21, 14,  6, 13, 16, 11, 19, 12, 20, 18,  5, 11,  6, 11,  6, 11, 21, 14,\n",
      "         6, 13, 16, 11, 19, 12, 20, 18,  5, 11,  6, 11,  6, 11, 21, 14,  6, 13,\n",
      "        16, 11, 19, 12, 20, 18,  5, 11,  6, 11,  6, 11, 21, 14,  6, 13, 16, 11,\n",
      "        19, 12, 24, 11, 19, 12, 20, 11,  6, 14,  6, 11, 21, 14,  6, 13, 16, 11,\n",
      "        19, 12, 20, 38,  6, 11, 21, 14,  6, 28, 21, 14,  6, 13, 16, 11, 19, 12,\n",
      "        20, 11,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 18,  5, 11, 30, 11,\n",
      "        21, 14,  6, 13, 16, 11, 19, 12, 20, 11,  6, 11, 21, 14,  6, 13, 16, 11,\n",
      "        19, 12, 20, 39,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 24, 11, 19, 12,\n",
      "        20, 11,  6, 14,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 33,  6, 11,\n",
      "        21, 14,  6, 13, 16, 11, 19, 12, 20, 23,  5, 14, 22, 14,  6, 11, 21, 14,\n",
      "         6, 13, 16, 11, 19, 12, 20, 18,  5, 14, 22, 14,  6, 11, 21, 14,  6, 13,\n",
      "        16, 11, 19, 12, 20, 18,  5, 13, 22, 14,  6, 11, 21, 14,  6, 29, 16, 11,\n",
      "        19, 12, 20, 35,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 18,  5, 14,\n",
      "        22, 14,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 23,  5, 13, 22, 14,\n",
      "         6, 11, 21, 14,  6, 29, 16, 11, 19, 12, 20, 35,  6, 11, 21, 14,  6, 13,\n",
      "        16, 11, 19, 12, 24, 11, 19, 12, 20, 11,  6, 14,  6, 11, 21, 14,  6, 13,\n",
      "        16, 11, 19, 12, 20, 31,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 18,\n",
      "         5, 14, 22, 14,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 23,  5, 14,\n",
      "        22, 14,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 18,  5, 11, 30, 11,\n",
      "        21, 14,  6, 13, 16, 11, 19, 12, 20, 11,  6, 11, 21, 14,  6, 13, 16, 11,\n",
      "        19, 12, 24, 11, 19, 12, 20, 11,  6, 14,  6, 11, 21, 14,  6, 13, 16, 11,\n",
      "        19, 12, 20, 18,  5, 11, 30, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 33,\n",
      "         6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 38,  6, 11, 21, 14,  6, 13,\n",
      "        16, 11, 19, 12, 20, 31,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 18,\n",
      "         5, 11,  6, 11,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 18,  5, 14,\n",
      "        22, 14,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 18,  5, 14, 22, 14,\n",
      "         6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 18,  5, 11, 30, 11, 21, 14,\n",
      "         6, 13, 16, 11, 19, 12, 20, 23,  5, 14, 22, 14,  6, 11, 21, 14,  6, 13,\n",
      "        16, 11, 19, 12, 20, 38,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 34,\n",
      "         5, 13, 22, 13,  6, 11, 21, 14,  6, 28, 21, 14,  6, 28, 21, 14,  6, 13,\n",
      "        16, 11, 19, 12, 20, 18,  5, 14, 22, 14,  6, 11, 21, 14,  6, 13, 16, 11,\n",
      "        19, 12, 20, 18,  5, 11,  6, 11,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12,\n",
      "        20, 25,  9, 18,  9, 11,  5, 14,  6, 11,  9, 27,  6, 11, 21, 14,  6, 13,\n",
      "        16, 11, 19, 12, 20, 31,  6, 11, 21, 14,  6, 28, 21, 14,  6, 13, 16, 11,\n",
      "        19, 12, 20, 23,  5, 14, 22, 14,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12,\n",
      "        20, 18,  5, 14, 22, 14,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 18,\n",
      "         5, 11,  6, 11,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 24, 11, 19, 12,\n",
      "        20, 11,  6, 18,  6, 11, 21, 14,  6, 13, 16, 11, 19, 12, 20, 33,  6, 11,\n",
      "        21, 14,  6, 13, 16, 11, 19, 12, 20, 33,  6, 11, 21, 14,  6, 13, 16, 11,\n",
      "        19, 12, 20, 18,  5, 11, 30, 11, 21, 14,  6, 14,  2])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import params\n",
    "\n",
    "a = list(params.SP_LABELS.keys()).insert(0, 'metrics')\n",
    "b = list(params.SP_LABELS.keys())\n",
    "b.insert(0, 'metrics')\n",
    "print(a)"
   ],
   "id": "2a9959d4ff41e26c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "p = torch.tensor([[1, 2, 3],\n",
    "                  [1, 2, 5],\n",
    "                  [4, 1, 1],\n",
    "                  [2, 3, 2],\n",
    "                  [2, 4, 2]], dtype=torch.float)\n",
    "t = torch.tensor([2, 1, 0, 1, 2])\n",
    "t2 = torch.tensor([2, 2, 0, 1, 2])\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "f1 = F1Score(task='multiclass', num_classes=3, average=None)\n",
    "\n",
    "print(f1(p, t2))\n",
    "print(f1_score(torch.argmax(p, dim=1).tolist(), t2.tolist(), average=None, zero_division=np.nan))\n",
    "print(classification_report(torch.argmax(p, dim=1).tolist(), t2.tolist()))\n",
    "\n"
   ],
   "id": "ab0bca4938b4f0aa",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "cm = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [2, 4, 5],\n",
    "    [3, 1, 2],\n",
    "])\n",
    "\n",
    "tp = torch.diag(cm)\n",
    "fp = torch.sum(cm, dim=0) - tp\n",
    "fn = torch.sum(cm, dim=1) - tp\n",
    "tn = torch.sum(cm) - (tp + fn + fp)\n",
    "\n",
    "denom = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "torch.sqrt(denom)\n",
    "# math.sqrt(denom)\n",
    "\n",
    "# print((tp * tn + fp * fn) / math.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)))"
   ],
   "id": "3bc644399e2a2207",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n = torch.tensor([1, 2])\n",
    "d = torch.tensor([2, 0])\n",
    "\n",
    "z = torch.tensor(2)\n",
    "if z == 0:\n",
    "    print('true')\n",
    "\n",
    "for i, j in zip(n, d):\n",
    "    if j == 0:\n",
    "        print(0)\n",
    "    else:\n",
    "        print((i / j).item())"
   ],
   "id": "57e0ee9badbf4970",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T18:26:57.342328Z",
     "start_time": "2024-05-17T18:26:57.007719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data.sp_dataset import SPDataset\n",
    "from data.data_utils import SPBatchRandomSampler\n",
    "import utils as ut\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_paths = [f'data/sp_data/train_set_partition_0.json', f'data/sp_data/train_set_partition_1.json']\n",
    "train_set = SPDataset(json_paths=ut.abspaths(train_paths), data_type='aa')\n",
    "# torch.manual_seed(0)\n",
    "sampler = SPBatchRandomSampler(train_set, batch_size=2, current_epoch=0, shuffle=True)\n",
    "train_dataloader = DataLoader(train_set, batch_sampler=sampler)\n",
    "for idx, batch in enumerate(train_dataloader):\n",
    "    pass\n",
    "    # print(idx, batch)"
   ],
   "id": "6a54d4e1a82499cb",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T18:26:46.707471Z",
     "start_time": "2024-05-17T18:26:46.608476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "train_paths = [f'data/sp_data/train_set_partition_0.json', f'data/sp_data/train_set_partition_1.json']\n",
    "train_set = SPDataset(json_paths=ut.abspaths(train_paths), data_type='aa')\n",
    "print(range(3))\n",
    "subset = torch.utils.data.Subset(train_set, range(3))\n",
    "print(len(subset))"
   ],
   "id": "18a9ee24b1e9285f",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T17:28:21.941116Z",
     "start_time": "2024-05-18T17:28:21.936626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = ((1, 2, 3), 4, 5)\n",
    "b = ((4, 2, 3), 5, 7)\n",
    "c = ((4, 6, 2), 4, 1)\n",
    "batch = [a, b, c]\n",
    "x, y, z = map(list, zip(*batch))\n",
    "print(x, y, z)"
   ],
   "id": "176e84921ae8742a",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:12:17.331161Z",
     "start_time": "2024-05-21T08:12:17.315773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dgl.nn.pytorch import GraphConv\n",
    "import dgl\n",
    "import torch\n",
    "\n",
    "num_nodes = 10\n",
    "\n",
    "a = [0, 1, 2, 3, 4, 3]\n",
    "b = [1, 0, 3, 2, 3, 4]\n",
    "a1 = torch.zeros((num_nodes, num_nodes))\n",
    "for i, j in zip(a, b):\n",
    "    a1[i, j] = 1\n",
    "g1 = dgl.graph((a, b), num_nodes=num_nodes)\n",
    "g1 = dgl.add_self_loop(g1)\n",
    "g1.ndata['h'] = a1\n",
    "\n",
    "c = [0, 1, 2, 3, 6, 3]\n",
    "d = [1, 0, 3, 2, 3, 6]\n",
    "a2 = torch.zeros((num_nodes, num_nodes))\n",
    "for i, j in zip(c, d):\n",
    "    a2[i, j] = 1\n",
    "g2 = dgl.graph((c, d), num_nodes=num_nodes)\n",
    "g2 = dgl.add_self_loop(g2)\n",
    "g2.ndata['h'] = a2\n",
    "\n",
    "batch = dgl.batch([g1, g2])\n",
    "# feats = torch.concat([a1, a2])\n",
    "\n",
    "conv = GraphConv(in_feats=num_nodes, out_feats=2, allow_zero_in_degree=True)\n",
    "ret = conv(batch, batch.ndata['h'])\n",
    "# ret = conv(g2, a2)\n",
    "print(torch.reshape(ret, (-1, num_nodes, 2)).shape)"
   ],
   "id": "2c82bddd6ef04e72",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T03:19:52.284535Z",
     "start_time": "2024-05-21T03:19:50.739055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dgl.data import MiniGCDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# A dataset with 80 samples, each graph is\n",
    "# of size [10, 20]\n",
    "dataset = MiniGCDataset(80, 10, 20)\n",
    "graph, label = dataset[0]\n",
    "fig, ax = plt.subplots()\n",
    "nx.draw(graph.to_networkx(), ax=ax)\n",
    "ax.set_title('Class: {:d}'.format(label))\n",
    "plt.show()"
   ],
   "id": "23d172315df79c27",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:13:39.063793Z",
     "start_time": "2024-06-03T18:13:39.054999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6]\n",
    "    ],\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [0, 0, 0]\n",
    "    ]\n",
    "])\n",
    "\n",
    "t2 = torch.tensor([\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [1,2,3]\n",
    "])\n",
    "\n",
    "split_ids = torch.tensor([2, 1])\n",
    "\n",
    "d_model = 3\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def split_batch(batch, split_ids, max_len):\n",
    "    tmp = iter(batch)\n",
    "    res = [list(islice(tmp, 0, ele)) for ele in split_ids]\n",
    "    print(res)\n",
    "    ret = []\n",
    "    if max_len == \"longest\":\n",
    "        max_len = torch.max(split_ids)\n",
    "        # print(max_len)\n",
    "    # if not isinstance(max_len, int):\n",
    "    #     raise ValueError('Use `int` or \"longest\"')\n",
    "    for ele in res:\n",
    "        print(ele)\n",
    "        if len(ele) > max_len:\n",
    "            ret.append(torch.stack(ele[:max_len]))\n",
    "        else:\n",
    "            while len(ele) < max_len:\n",
    "                pad = torch.zeros(d_model)\n",
    "                ele.append(pad)\n",
    "            ret.append(torch.stack(ele))\n",
    "    return torch.stack(ret)\n",
    "print(split_batch(t2, split_ids, \"longest\"))"
   ],
   "id": "7f210d9e34efa946",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor([1, 2, 3]), tensor([4, 5, 6])], [tensor([1, 2, 3])]]\n",
      "[tensor([1, 2, 3]), tensor([4, 5, 6])]\n",
      "[tensor([1, 2, 3])]\n",
      "tensor([[[1., 2., 3.],\n",
      "         [4., 5., 6.]],\n",
      "\n",
      "        [[1., 2., 3.],\n",
      "         [0., 0., 0.]]])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T18:29:11.728453Z",
     "start_time": "2024-06-01T18:29:01.765039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([\n",
    "    [1., 1., 1.],\n",
    "    [1., 0., 0.]\n",
    "])\n",
    "\n",
    "t = t.masked_fill(t == 0, float('-inf'))\n",
    "print(t)\n",
    "t = t.masked_fill(t == 1, float(0.0))\n",
    "print(t)"
   ],
   "id": "21007881574fc635",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., -inf, -inf]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., -inf, -inf]])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:11:55.005311Z",
     "start_time": "2024-06-02T17:11:54.992924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "t = torch.zeros((8, 4))\n",
    "t[:, -1] = float('-inf')\n",
    "print(t)"
   ],
   "id": "2970c4e1e2d6b39e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., -inf]])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:14:21.721478Z",
     "start_time": "2024-06-03T18:14:21.716794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "t1 = torch.tensor([0,0,0])\n",
    "t2 = [\n",
    "    torch.tensor([1,0,0]),\n",
    "    torch.tensor([0,1,0]),\n",
    "]\n",
    "print(torch.stack([t1, *t2]))"
   ],
   "id": "34dea656ad9b65f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [0, 1, 0]])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:21:44.596574Z",
     "start_time": "2024-06-03T18:21:44.580395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "t = [1,2,3]\n",
    "t[len(t)]"
   ],
   "id": "12bb530825ee2dab",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m t \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m3\u001B[39m]\n\u001B[1;32m----> 2\u001B[0m \u001B[43mt\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T18:47:22.810295Z",
     "start_time": "2024-06-12T18:47:22.790735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import utils as ut\n",
    "import params\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(ut.abspath(f'out/metrics/ap_score.csv'))\n",
    "df_total = pd.read_csv(ut.abspath(f'out/metrics/ap_score_TOTAL.csv'))\n",
    "\n",
    "models = df_total['model_type'].unique()\n",
    "num_models = len(models)\n",
    "\n",
    "ranking_table = np.zeros((len(params.ORGANISMS), num_models, num_models))\n",
    "ranking_table_total = np.zeros((num_models, num_models))\n",
    "\n",
    "# for TOTAL\n",
    "for k, l in params.SP_LABELS.items():\n",
    "    ranks = np.array(df_total[k].rank(method='min', ascending=False), dtype=int)\n",
    "    for i, rank in enumerate(ranks):\n",
    "        ranking_table_total[i][rank-1] += 1\n",
    "\n",
    "for k1, o in params.ORGANISMS.items():\n",
    "    for k2, l in params.SP_LABELS.items():\n",
    "        if df[df['organism'] == k1][k2].sum() != 0.0:\n",
    "            ranks = np.array(df[df['organism'] == k1][k2].rank(method='min', ascending=False), dtype=int)\n",
    "            for i, rank in enumerate(ranks):\n",
    "                ranking_table[o][i][rank-1] += 1\n",
    "            \n",
    "print(ranking_table)\n",
    "print(ranking_table_total[15])\n",
    "# df[df['organism'] == \"EUKARYA\"]\n",
    "# for k, org in params.ORGANISMS.items():"
   ],
   "id": "dd263ae952521f6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_default, smiles, Using Organism: True\n",
      "[3. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T09:38:49.251055Z",
     "start_time": "2024-06-18T09:38:31.249029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Bio import SeqIO\n",
    "import data.data_utils as dut\n",
    "from visualization import _statistics_on_total, _statistics_on_train_val_test\n",
    "import json\n",
    "\n",
    "records = SeqIO.parse(dut.SIGNALP6_PATH, 'fasta')\n",
    "statistics_on_organisms, statistics_on_labels, statistics_on_labels_organism = _statistics_on_total(records)\n",
    "\n",
    "# extract train/val/test set\n",
    "train_paths = [f'data/sp_data/train_set_partition_0.json', f'data/sp_data/train_set_partition_1.json']\n",
    "val_paths = [f'data/sp_data/test_set_partition_0.json', f'data/sp_data/test_set_partition_1.json']\n",
    "test_paths = [f'data/sp_data/train_set_partition_2.json', f'data/sp_data/test_set_partition_2.json']\n",
    "\n",
    "train_records, val_records, test_records = [], [], []\n",
    "for train_path, val_path, test_path in zip(train_paths, val_paths, test_paths):\n",
    "    with open(train_path, 'r') as f:\n",
    "        train_records.extend(json.load(f))\n",
    "    with open(val_path, 'r') as f:\n",
    "        val_records.extend(json.load(f))\n",
    "    with open(test_path, 'r') as f:\n",
    "        test_records.extend(json.load(f))\n",
    "\n",
    "statistics_on_train_organisms, statistics_on_train_labels, statistics_on_train_labels_organism = _statistics_on_train_val_test(\n",
    "    train_records)\n",
    "statistics_on_val_organisms, statistics_on_val_labels, statistics_on_val_labels_organism = _statistics_on_train_val_test(\n",
    "    val_records)\n",
    "statistics_on_test_organisms, statistics_on_test_labels, statistics_on_test_labels_organism = _statistics_on_train_val_test(\n",
    "    test_records)"
   ],
   "id": "666b60af9ad275c5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T09:40:26.054444Z",
     "start_time": "2024-06-18T09:40:26.049759Z"
    }
   },
   "cell_type": "code",
   "source": "statistics_on_train_labels_organism",
   "id": "911c009ad540caa0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EUKARYA': {'NO_SP': 8603, 'SP': 1230},\n",
       " 'POSITIVE': {'NO_SP': 97,\n",
       "  'SP': 80,\n",
       "  'LIPO': 379,\n",
       "  'TAT': 29,\n",
       "  'PILIN': 2,\n",
       "  'TATLIPO': 1},\n",
       " 'NEGATIVE': {'NO_SP': 396,\n",
       "  'SP': 181,\n",
       "  'LIPO': 606,\n",
       "  'TAT': 188,\n",
       "  'PILIN': 45,\n",
       "  'TATLIPO': 13},\n",
       " 'ARCHAEA': {'NO_SP': 52,\n",
       "  'SP': 24,\n",
       "  'LIPO': 7,\n",
       "  'TAT': 9,\n",
       "  'PILIN': 9,\n",
       "  'TATLIPO': 5}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T10:03:19.179337Z",
     "start_time": "2024-06-18T10:03:19.171177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import params\n",
    "import pandas as pd\n",
    "import utils as ut\n",
    "orgs = []\n",
    "lbs = [[], [], [], [], [], []]\n",
    "\n",
    "for org in params.ORGANISMS.keys():\n",
    "    orgs.append(org)\n",
    "    for l, i in params.SP_LABELS.items():\n",
    "        if l in statistics_on_test_labels_organism[org].keys():\n",
    "            lbs[i].append(statistics_on_test_labels_organism[org][l])\n",
    "        else:\n",
    "            lbs[i].append(0)\n",
    "        \n",
    "dt = {\n",
    "    \"orgs\": orgs,\n",
    "    \"NO_SP\": lbs[0],\n",
    "    \"SP\": lbs[1],\n",
    "    \"LIPO\": lbs[2],\n",
    "    \"TAT\": lbs[3],\n",
    "    \"PILIN\": lbs[4],\n",
    "    \"TATLIPO\": lbs[5]\n",
    "}\n",
    "df = pd.DataFrame(dt)\n",
    "df.to_csv(ut.abspath(f'out/metrics/statistics_on_test_lbs_orgs.csv'), index=False, na_rep=0)"
   ],
   "id": "1eb895081d97a37c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(statistics_on_labels).transpose()\n",
    "df.to_csv(ut.abspath(f'out/metrics/statistics_on_labels.csv.csv'))"
   ],
   "id": "f696f8d54e037f7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f6307d64dfdb290b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T00:51:17.114160Z",
     "start_time": "2024-06-20T00:51:17.104332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "# y_true = np.array([0, 0, 1, 1])\n",
    "# y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "# average_precision_score(y_true, y_scores)\n",
    "y_true = np.array([0, 0, 1, 1, 2, 2])\n",
    "y_scores = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.4, 0.3, 0.3],\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.2, 0.3, 0.5],\n",
    "    [0.4, 0.4, 0.2],\n",
    "    [0.1, 0.2, 0.7],\n",
    "])\n",
    "\n",
    "print(y_true.shape, y_scores.shape)\n",
    "average_precision_score(y_true, y_scores)"
   ],
   "id": "8b93a4090d5b34d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,) (6, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7777777777777777"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T08:09:30.573627Z",
     "start_time": "2024-06-21T08:09:30.564584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import utils as ut\n",
    "import pandas as pd\n",
    "\n",
    "model = \"cnn-aa-default-0_epochs=100\"\n",
    "\n",
    "file_org_loc = ut.abspath(f'out/metrics/{model}_ap_score_ORG.csv')\n",
    "file_total_loc = ut.abspath(f'out/metrics/{model}_ap_score_TOTAL.csv')\n",
    "\n",
    "ap_org = pd.read_csv(file_org_loc)\n",
    "ap_total = pd.read_csv(file_total_loc)\n",
    "\n",
    "ap_org[ap_org[\"index\"] == 0]['model'].values[0]\n",
    "# ap_total"
   ],
   "id": "f54772fae1506d3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CNN, AA Seq, Organism: No'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T18:48:22.213143Z",
     "start_time": "2024-06-22T18:48:12.092125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "import data.data_utils as dut\n",
    "from Bio import SeqIO\n",
    "import json\n",
    "import torch\n",
    "\n",
    "tokenizer_path = ut.abspath(f'tokenizer/tokenizer_smiles.json')\n",
    "tokenizer = GPT2TokenizerFast(tokenizer_file=tokenizer_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "records = SeqIO.parse(dut.SIGNALP6_PATH, 'fasta')\n",
    "max_len = [0, 0]\n",
    "min_len = [1000, 10000]\n",
    "# for record in records:\n",
    "#     seq = str(record.seq)[:len(record.seq) // 2]\n",
    "#     if len(seq) > max_len[0]:\n",
    "#         max_len[0] = len(seq)\n",
    "#     if len(seq) < min_len[0]:\n",
    "#         min_len[0] = len(seq)\n",
    "# print(max_len[0], min_len[0])\n",
    "\n",
    "train_paths = [f'data/sp_data/train_set_partition_0.json', f'data/sp_data/train_set_partition_1.json']\n",
    "val_paths = [f'data/sp_data/test_set_partition_0.json', f'data/sp_data/test_set_partition_1.json']\n",
    "test_paths = [f'data/sp_data/train_set_partition_2.json', f'data/sp_data/test_set_partition_2.json']\n",
    "\n",
    "train_records, val_records, test_records = [], [], []\n",
    "for train_path, val_path, test_path in zip(train_paths, val_paths, test_paths):\n",
    "    with open(train_path, 'r') as f:\n",
    "        train_records.extend(json.load(f))\n",
    "    with open(val_path, 'r') as f:\n",
    "        val_records.extend(json.load(f))\n",
    "    with open(test_path, 'r') as f:\n",
    "        test_records.extend(json.load(f))\n",
    "\n",
    "for record in train_records:\n",
    "    aa = record['aa_seq']\n",
    "    encoded = tokenizer.batch_encode_plus([record['smiles']])\n",
    "    smiles = torch.tensor(encoded['input_ids'][0])\n",
    "    if len(aa) > max_len[0]:\n",
    "        max_len[0] = len(aa)\n",
    "    if len(aa) < min_len[0]:\n",
    "        min_len[0] = len(aa)\n",
    "    if len(smiles) > max_len[1]:\n",
    "        max_len[1] = len(smiles)\n",
    "    if len(smiles) < min_len[1]:\n",
    "        min_len[1] = len(smiles)\n",
    "\n",
    "for record in val_records:\n",
    "    aa = record['aa_seq']\n",
    "    encoded = tokenizer.batch_encode_plus([record['smiles']])\n",
    "    smiles = torch.tensor(encoded['input_ids'][0])\n",
    "    if len(aa) > max_len[0]:\n",
    "        max_len[0] = len(aa)\n",
    "    if len(aa) < min_len[0]:\n",
    "        min_len[0] = len(aa)\n",
    "    if len(smiles) > max_len[1]:\n",
    "        max_len[1] = len(smiles)\n",
    "    if len(smiles) < min_len[1]:\n",
    "        min_len[1] = len(smiles)\n",
    "\n",
    "# for record in test_records:\n",
    "#     smiles = record['smiles']\n",
    "#     if len(smiles) > max_len[1]:\n",
    "#         max_len[1] = len(smiles)\n",
    "#     if len(smiles) < min_len[1]:\n",
    "#         min_len[1] = len(smiles)\n",
    "\n",
    "data = {\n",
    "    \"data_type\": [\"AA Seq\", \"SMILES\"],\n",
    "    \"min_len\": min_len,\n",
    "    \"max_len\": max_len,\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "import utils as ut\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(ut.abspath(f'out/metrics/statistic_on_seq_len.csv'), index=False, na_rep=0)\n",
    "\n",
    "    "
   ],
   "id": "c08bdfe0b665c4bf",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "420410bf2495bf02"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
