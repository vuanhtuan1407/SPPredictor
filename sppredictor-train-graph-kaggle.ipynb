{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 8167734,
     "sourceType": "datasetVersion",
     "datasetId": 4833366
    },
    {
     "sourceId": 8371197,
     "sourceType": "datasetVersion",
     "datasetId": 4833354
    },
    {
     "sourceId": 8394093,
     "sourceType": "datasetVersion",
     "datasetId": 4833361
    },
    {
     "sourceId": 8606734,
     "sourceType": "datasetVersion",
     "datasetId": 5100285
    },
    {
     "sourceId": 8596898,
     "sourceType": "datasetVersion",
     "datasetId": 4833274
    }
   ],
   "dockerImageVersionId": 30699,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# import os\n# for dir, _, files in os.walk('kaggle/input'):",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# !pip list\n# !nvidia-smi\n# !sudo find / -name 'libcudart.so.11.0'\n# !conda install cudatoolkit\n# %cd /usr/local/cuda-12.1/targets/x86_64-linux/lib\n# %ls",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "## install torch 2.2.1 and cuda 12.1\n!pip install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121\n!pip install lightning\n# !pip install  dgl -f https://data.dgl.ai/wheels/torch-2.1/cu118/repo.html\n!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.2/cu121/repo.html\n# !pip install wandb\n# !pip install scikit-learn\n# !pip install transformers\n\n## install for torch 2.1.0 and cuda 11.8\n# !pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n# !pip install lightning\n# !pip install  dgl -f https://data.dgl.ai/wheels/torch-2.1/cu118/repo.html",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Export ENV",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%env DGLBACKEND=pytorch",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Import constants in param.py",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# import os\nimport platform\nfrom typing import Literal\n\nOS_PLATFORM = platform.system()\n\nSP_LABELS = dict(NO_SP=0, SP=1, LIPO=2, TAT=3, PILIN=4, TATLIPO=5)\nORGANISMS = dict(EUKARYA=0, POSITIVE=1, NEGATIVE=2, ARCHAEA=3)\n\n\"\"\"\nDATA PREPARATION\n\"\"\"\nUSE_PREPARED_DATA = False\nTRAIN_PATH = 'data/sp_data/train_set.fasta'\nBENCHMARK_PATH = 'data/sp_data/benchmark_set_sp5.fasta'\n\nUSE_SPLIT_DATASET = False\nON_ORGANISM: Literal['eukarya', 'others'] = 'others'  # use when you set `USE_SPLIT_DATASET=True`\n\n\"\"\"\nMODEL AND TRAINER CONFIGURATION\n\"\"\"\n# Training\nMODEL_TYPE = \"gconv\"\nDATA_TYPE: Literal['aa', 'smiles', 'graph'] = 'graph'\nCONF_TYPE = 'default'\nEPOCHS = 100\n# ENV = 'kaggle'\nUSE_ORGANISM = True\n\n# MODEL_TYPE = \"transformer\"\n# DATA_TYPE: Literal['aa', 'smiles', 'graph'] = 'aa'\n# CONF_TYPE = 'default'\n# EPOCHS = 1\n# # ENV = 'kaggle'\n# USE_ORGANISM = True\n\nBATCH_SIZE = 8\nLEARNING_RATE = 1e-7\nNUM_WORKERS = 0  # set to 0 because of some random_seeding reason\nFREEZE_PRETRAINED = False\n\nDEVICES: list[int] | str | int = 'auto'\nACCELERATOR = 'auto'\n\nENABLE_CHECKPOINTING = True\n\n# Testing\nCHECKPOINT: str = \"bert_pretrained-aa-default-1_epochs=100.ckpt\"\n\n# DEVICE = 'cpu'  # use when applying old training process\n\n\"\"\"\nLOGGER CONFIGURATION\n\"\"\"\nUSE_LOGGER = False\nINPUT_DIR = '/kaggle/input'\nWORKING_DIR = '/kaggle/working'\n\nLOG_DIR = 'logs'  # relative path",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Directory _callbacks_",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# import all callback in callback_utils.py here\nfrom pathlib import Path\nfrom lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n\nfilename = f'{MODEL_TYPE}-{DATA_TYPE}-{CONF_TYPE}-{int(USE_ORGANISM)}_epochs={EPOCHS}'\nmodel_checkpoint = ModelCheckpoint(\n    dirpath=str(Path(WORKING_DIR, 'checkpoints')),\n    filename=filename,\n    enable_version_counter=True,\n    monitor='val_loss',\n    every_n_epochs=1,\n    save_on_train_epoch_end=True,\n    mode='min',\n    save_top_k=1,\n)  # return location: ~/checkpoints/<model>-<data>-<conf>-<used_org>_epochs=<epochs>[_v<ver>].ckpt\nmodel_checkpoint.CHECKPOINT_JOIN_CHAR = '_'\n\nearly_stopping = EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=0.00,\n    patience=11,\n    verbose=True,\n    check_finite=True,\n    mode=\"min\"\n)\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Directory _configs_",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# import all utils in config_utils.py\nimport json\n\nfrom transformers import BertConfig\n\n\ndef load_config(model, data, conf_type):\n    if model == \"bert_pretrained\":\n        config = BertConfig().from_pretrained(\"Rostlab/prot_bert\")\n        return config\n    elif model == \"bert\":\n        with open(str(Path(INPUT_DIR, f'sppredictor-config-{data}', f'{model}_config_default.json'))) as f:\n            data = json.load(f)\n            config = BertConfig(**data)\n            return config\n    else:\n        conf_path = str(Path(INPUT_DIR, f'sppredictor-config-{data}', f'{model}_config_{conf_type}.json'))\n        if os.path.exists(conf_path):\n            with open(conf_path, 'r') as f:\n                config = json.load(f)\n                return config\n        else:\n            raise FileNotFoundError(\"Config file does not exist\")\n\n\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Metrics",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from typing import Any, Literal, Optional\n\nimport torch\nfrom torchmetrics import MatthewsCorrCoef, Metric\nfrom torchmetrics.classification import MulticlassMatthewsCorrCoef, BinaryMatthewsCorrCoef, MultilabelMatthewsCorrCoef\nfrom torchmetrics.utilities.enums import ClassificationTask\n\n\ndef _matthews_corrcoef_non_average(confmat: torch.Tensor):\n    mcc = []\n    tps = torch.diag(confmat)\n    fps = torch.sum(confmat, dim=0) - tps\n    fns = torch.sum(confmat, dim=1) - tps\n    tns = torch.sum(confmat) - (tps + fns + fps)\n\n    for tp, fp, fn, tn in zip(tps, fps, fns, tns):\n        numerator = (tp * tn - fp * fn)\n        denominator = torch.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n        if denominator == 0:\n            mcc.append(0)\n        else:\n            mcc.append((numerator / denominator).item())\n    return torch.tensor(mcc, device=confmat.device)\n\n\nclass MulticlassMatthewsCorrCoefNoneAverage(MulticlassMatthewsCorrCoef):\n    def compute(self) -> Any:\n        return _matthews_corrcoef_non_average(self.confmat)\n\n\nclass MCC(MatthewsCorrCoef):\n    def __new__(  # type: ignore[misc]\n            cls,\n            task: Literal[\"binary\", \"multiclass\", \"multilabel\"],\n            threshold: float = 0.5,\n            num_classes: Optional[int] = None,\n            num_labels: Optional[int] = None,\n            average: Literal[\"micro\"] | None = 'micro',\n            ignore_index: Optional[int] = None,\n            validate_args: bool = True,\n            **kwargs: Any,\n    ) -> Metric:\n        \"\"\"Initialize task and average metric.\"\"\"\n        task = ClassificationTask.from_str(task)\n        kwargs.update({\"ignore_index\": ignore_index, \"validate_args\": validate_args})\n        if average == \"micro\":\n            if task == ClassificationTask.BINARY:\n                return BinaryMatthewsCorrCoef(threshold, **kwargs)\n            if task == ClassificationTask.MULTICLASS:\n                if not isinstance(num_classes, int):\n                    raise ValueError(f\"`num_classes` is expected to be `int` but `{type(num_classes)} was passed.`\")\n                return MulticlassMatthewsCorrCoef(num_classes, **kwargs)\n            if task == ClassificationTask.MULTILABEL:\n                if not isinstance(num_labels, int):\n                    raise ValueError(f\"`num_labels` is expected to be `int` but `{type(num_labels)} was passed.`\")\n                return MultilabelMatthewsCorrCoef(num_labels, threshold, **kwargs)\n            raise ValueError(f\"Not handled value: {task}\")\n        else:\n            # if task == ClassificationTask.BINARY:\n            #     return BinaryMatthewsCorrCoefNoneAverage(threshold, **kwargs)\n            if task == ClassificationTask.MULTICLASS:\n                if not isinstance(num_classes, int):\n                    raise ValueError(f\"`num_classes` is expected to be `int` but `{type(num_classes)} was passed.`\")\n                return MulticlassMatthewsCorrCoefNoneAverage(num_classes, **kwargs)\n            # if task == ClassificationTask.MULTILABEL:\n            #     if not isinstance(num_labels, int):\n            #         raise ValueError(f\"`num_labels` is expected to be `int` but `{type(num_labels)} was passed.`\")\n            #     return MultilabelMatthewsCorrCoefNoneAverage(num_labels, threshold, **kwargs)\n            raise ValueError(f\"Not handled value: {task}\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Directory _models_",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from dgl.nn.pytorch import GraphConv\nfrom itertools import islice\n# Neural Network Layers\n\nimport math\n\nfrom torch import nn\n\n\nclass OrganismEmbedding(nn.Module):\n    def __init__(self, num_orgs: int = 4, e_dim: int = 512):\n        super().__init__()\n        self.num_orgs = num_orgs\n        self.embedding_dim = e_dim\n        torch.random.manual_seed(0)\n        oe = torch.randn(num_orgs, e_dim)\n        self.organism_embedding = nn.Embedding.from_pretrained(oe, freeze=True)\n\n    def forward(self, x):\n        return self.organism_embedding(x)\n\n\nclass InputEmbedding(nn.Module):\n    def __init__(self, vocab_size: int = 100, d_model: int = 512):\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.input_embedding = nn.Embedding(\n            num_embeddings=vocab_size,\n            embedding_dim=d_model\n        )\n\n    def forward(self, x):\n        x = self.input_embedding(x) * math.sqrt(self.d_model)\n        return x\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int = 512, dropout: float = 0.1, max_len: int = 2048):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, d_model)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        pe_x = self.pe[:, :x.size(1), :]\n        x = x + pe_x\n        return self.dropout(x)\n\n\nclass LinearPositionalEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int = 512, dropout: float = 0.1):\n        super().__init__()\n        self.pe = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x):\n        return self.pe(x)\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, d_model: int = 512, nhead: int = 8, num_layers: int = 6):\n        super().__init__()\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(\n            encoder_layer=encoder_layer,\n            num_layers=num_layers\n        )\n\n    def forward(self, x, mask=None):\n        x = self.encoder(x, src_key_padding_mask=mask)\n        # use average [CLS] token with all other word tokens\n        # x = torch.mean(x, dim=1)\n\n        # use only [CLS] token\n        x = x[:, 0, :]\n        return x\n\n\nclass ConvolutionalEncoder(nn.Module):\n    def __init__(\n            self,\n            embedding_dim: int = 512,\n            dropout: float = 0.1,\n            kernel_size: int = 3,\n            stride: int = 1,\n            padding: int = 0,\n            n_base: int = 1024,\n    ):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Dropout(p=dropout),\n            nn.Conv1d(in_channels=embedding_dim, out_channels=n_base, kernel_size=kernel_size, stride=stride,\n                      padding=padding),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=3, stride=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Dropout(p=dropout),\n            nn.Conv1d(in_channels=n_base, out_channels=n_base * 4, kernel_size=kernel_size, stride=stride,\n                      padding=padding),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=3, stride=2)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Dropout(p=dropout),\n            nn.Conv1d(in_channels=n_base * 4, out_channels=n_base, kernel_size=kernel_size, stride=stride,\n                      padding=padding),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=3, stride=2)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Dropout(p=dropout),\n            nn.Conv1d(in_channels=n_base, out_channels=embedding_dim, kernel_size=kernel_size, stride=stride,\n                      padding=padding),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=3, stride=2)\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        return x\n\n\nclass LSTMEncoder(nn.Module):\n    def __init__(\n            self,\n            embedding_dim: int = 512,\n            hidden_size: int = 1024,\n            n_layers: int = 4,\n            dropout: float = 0.1,\n            random_init: bool = False\n    ):\n        super().__init__()\n        self.random_init = random_init\n        self.lstm = nn.LSTM(\n            input_size=embedding_dim,\n            hidden_size=hidden_size,\n            num_layers=n_layers,\n            dropout=dropout,\n            batch_first=True,\n            bidirectional=False\n        )\n\n    def forward(self, x):\n        out, (h_n, c_n) = self.lstm(x)\n        return out, h_n, c_n\n\n\nclass StackedBiLSTMEncoder(nn.Module):\n    def __init__(\n            self,\n            embedding_dim: int = 512,\n            hidden_size: int = 1024,\n            n_layers: int = 4,\n            dropout: float = 0.1,\n            random_init: bool = False\n    ):\n        super().__init__()\n        self.random_init = random_init\n        # Init state\n        if random_init:\n            (h_0, c_0) = self.__init_state(n_layers=n_layers, hidden_size=hidden_size)\n            self.init_state = (h_0, c_0)\n\n        self.bilstm = nn.LSTM(\n            input_size=embedding_dim,\n            hidden_size=hidden_size,\n            bidirectional=True,\n            num_layers=n_layers,\n            batch_first=True,\n            dropout=dropout\n        )\n\n    def forward(self, x):\n        if self.random_init:\n            out, (h_n, c_n) = self.bilstm(x, (self.init_state[0].detach(), self.init_state[1].detach()))\n            return out, h_n, c_n\n        else:\n            out, (h_n, c_n) = self.bilstm(x)\n            return out, h_n, c_n\n\n    @staticmethod\n    def __init_state(n_layers: int = 4, hidden_size: int = 1024):\n        h_0 = torch.zeros(n_layers * 2, BATCH_SIZE, hidden_size).requires_grad_(True)\n        c_0 = torch.zeros(n_layers * 2, BATCH_SIZE, hidden_size).requires_grad_(True)\n        return h_0, c_0\n\n\nclass ParallelBiLSTMEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        pass\n\n\nclass GraphConvEncoder(nn.Module):\n    def __init__(self, d_model: int = 512, n_layers=2, dropout: float = 0.1, use_relu_act: bool = True,\n                 d_hidden: int = 1024, use_special_tokens: bool = False):\n        super().__init__()\n        self.d_model = d_model\n        self.use_special_tokens = use_special_tokens\n        self.act = None\n        if use_relu_act:\n            self.act = nn.ReLU()\n        self.convs = nn.ModuleList()\n        convFirst = GraphConv(20, d_hidden, norm='both', bias=True, activation=self.act, allow_zero_in_degree=True)\n        self.convs.append(convFirst)\n\n        for i in range(1, n_layers - 1):\n            convIn = GraphConv(d_hidden, d_hidden, norm='both', bias=True, activation=self.act,\n                               allow_zero_in_degree=True)\n            self.convs.append(convIn)\n\n        convLast = GraphConv(d_hidden, d_model, norm='both', bias=True, activation=self.act,\n                             allow_zero_in_degree=True)\n        self.convs.append(convLast)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, h):\n        for (i, conv) in enumerate(self.convs):\n            h = conv(x, h)\n        # h = self.return_batch(h, x.batch_num_nodes(), max_len='longest')\n        if self.use_special_tokens:\n            h, mask = self.return_batch_plus(h, x.batch_num_nodes(), max_len='longest')\n            # h = torch.reshape(h, (-1, 20, self.d_model))\n            h = self.dropout(h)\n            return h, mask\n        else:\n            h = self.return_batch(h, x.batch_num_nodes(), max_len='longest')\n            h = self.dropout(h)\n            return h\n\n    def return_batch(self, h, batch_num_nodes, max_len: str | int = 70):\n        device = h.get_device()\n        tmp = [list(islice(iter(h), 0, num_nodes)) for num_nodes in batch_num_nodes]\n        ret = []\n        if max_len == \"longest\":\n            max_len = torch.max(batch_num_nodes).item()\n        if not isinstance(max_len, int):\n            raise ValueError('Use `int` or \"longest\"')\n\n        for i, sample in enumerate(tmp):\n            if len(sample) > max_len:\n                sample = sample[:max_len]\n            else:\n                while len(sample) < max_len:\n                    pad = torch.zeros(self.d_model, device=device)\n                    sample.append(pad)\n            ret.append(torch.stack(sample))\n\n        return torch.stack(ret)\n\n    def return_batch_plus(self, h, batch_num_nodes, max_len: str | int = 70):\n        device = h.get_device()\n        tmp = [list(islice(iter(h), 0, num_nodes)) for num_nodes in batch_num_nodes]\n        ret = []\n        if max_len == \"longest\":\n            max_len = torch.max(batch_num_nodes).item()\n        if not isinstance(max_len, int):\n            raise ValueError('Use `int` or \"longest\"')\n        mask = torch.zeros((len(batch_num_nodes), max_len + 2), dtype=torch.float, device=device)\n        mask[:, -1] = float('-inf')\n        for i, sample in enumerate(tmp):\n            if len(sample) > max_len:\n                sample = sample[:max_len]\n            else:\n                while len(sample) < max_len:\n                    pad = torch.zeros(self.d_model, device=device)\n                    sample.append(pad)\n                    mask[i, len(sample)] = float('-inf')\n            sample = self.add_special_tokens(sample, device)\n            ret.append(torch.stack(sample))\n\n        return torch.stack(ret), mask\n\n    def add_special_tokens(self, sample, device):\n        cls = torch.zeros(self.d_model, device=device)  # begin of sentence [CLS]\n        eos = torch.zeros(self.d_model, device=device)  # end of sentence [EOS]\n        return [cls, *sample, eos]\n\n\nclass Classifier(nn.Module):\n    def __init__(self, num_class: int, d_model: int = 512, d_ff: int = 2048):\n        super().__init__()\n        self.ff1 = nn.Linear(in_features=d_model, out_features=d_ff)\n        self.ff2 = nn.Linear(in_features=d_ff, out_features=num_class)\n        self.act1 = nn.ReLU()\n        # self.act2 = nn.Softmax()\n\n    def forward(self, x):\n        x = self.act1(self.ff1(x))\n        # x = self.act2(self.ff2(x))\n        x = self.ff2(x)\n        return x",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# models\n\nfrom torch import nn\n\n\"\"\" Transformer \"\"\"\n\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.input_embedding = InputEmbedding(\n            vocab_size=config['vocab_size'],\n            d_model=config['d_model']\n        )\n        self.positional_encoding = PositionalEncoding(\n            d_model=config['d_model'],\n            dropout=config['dropout'],\n            max_len=config['max_len']\n        )\n        self.encoder = TransformerEncoder(\n            d_model=config['d_model'],\n            nhead=config['nhead'],\n            num_layers=config['num_layers']\n        )\n        self.classifier = Classifier(\n            d_model=config['d_model'],\n            num_class=len(SP_LABELS)\n        )\n\n    def forward(self, x, mask=None):\n        x = self.input_embedding(x)\n        x = self.positional_encoding(x)\n        x = self.encoder(x, mask)\n        x = self.classifier(x)\n        return x\n\n\nclass TransformerOrganismClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.input_embedding = InputEmbedding(\n            vocab_size=config['vocab_size'],\n            d_model=config['d_model']\n        )\n        self.positional_encoding = PositionalEncoding(\n            d_model=config['d_model'],\n            dropout=config['dropout'],\n            max_len=config['max_len']\n        )\n        self.encoder = TransformerEncoder(\n            d_model=config['d_model'],\n            nhead=config['nhead'],\n            num_layers=config['num_layers']\n        )\n        self.organism_embedding = OrganismEmbedding(\n            num_orgs=len(ORGANISMS),\n            e_dim=config['d_model']\n        )\n        self.classifier = Classifier(\n            d_model=config['d_model'] * 2,\n            num_class=len(SP_LABELS)\n        )\n\n    def forward(self, x, org, mask=None):\n        x = self.input_embedding(x)\n        x = self.positional_encoding(x)\n        x = self.encoder(x, mask)\n        org = self.organism_embedding(org)\n        inp = torch.cat((x, org), dim=1)  # concat along model dim\n        out = self.classifier(inp)\n        return out\n\n\n\"\"\" CNN \"\"\"\n\n\nclass ConvolutionalClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.input_embedding = InputEmbedding(\n            vocab_size=config['vocab_size'],\n            d_model=config['d_model']\n        )\n        self.conv_encoder = ConvolutionalEncoder(\n            embedding_dim=config['d_model'],\n            kernel_size=config['kernel_size'],\n            n_base=config['n_base']\n        )\n        self.flatten = nn.Flatten()\n        self.classifier = Classifier(num_class=len(SP_LABELS), d_model=119808)\n        # self.dropout = nn.Dropout(p=0.1)\n\n    def forward(self, x):\n        x = self.input_embedding(x)\n        x = torch.transpose(x, 1, 2)\n        x = self.conv_encoder(x)\n        x = self.flatten(x)\n        x = self.classifier(x)\n        return x\n\n\nclass ConvolutionalOrganismClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.input_embedding = InputEmbedding(\n            vocab_size=config['vocab_size'],\n            d_model=config['d_model']\n        )\n        self.conv_encoder = ConvolutionalEncoder(\n            embedding_dim=config['d_model'],\n            kernel_size=config['kernel_size'],\n            n_base=config['n_base']\n        )\n        self.flatten = nn.Flatten()\n        self.organism_embedding = OrganismEmbedding(\n            num_orgs=len(ORGANISMS),\n            e_dim=config['d_model']\n        )\n        self.classifier = Classifier(num_class=len(SP_LABELS), d_model=120832)\n\n    def forward(self, x, org):\n        x = self.input_embedding(x)\n        x = torch.transpose(x, 1, 2)\n        x = self.conv_encoder(x)\n        x = torch.transpose(x, 1, 2)\n        org = self.organism_embedding(org)\n        org = org.unsqueeze(1)\n        inp = torch.cat((x, org), dim=1)\n        inp = self.flatten(inp)\n        out = self.classifier(inp)\n        return out\n\n\n\"\"\" LSTM \"\"\"\n\n\nclass LSTMClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.input_embedding = InputEmbedding(\n            vocab_size=config['vocab_size'],\n            d_model=config['d_model']\n        )\n        self.encoder = LSTMEncoder(\n            embedding_dim=config['d_model'],\n            hidden_size=config['hidden_size'],\n            n_layers=config['n_layers'],\n            dropout=config['dropout'],\n\n        )\n        self.classifier = Classifier(num_class=len(SP_LABELS), d_model=config['hidden_size'])\n\n    def forward(self, x):\n        x = self.input_embedding(x)\n        x, h_n, c_n = self.encoder(x)\n        x = self.classifier(x[:, -1, :])\n        return x\n\n\nclass LSTMOrganismClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.input_embedding = InputEmbedding(\n            vocab_size=config['vocab_size'],\n            d_model=config['d_model']\n        )\n        self.encoder = LSTMEncoder(\n            embedding_dim=config['d_model'],\n            hidden_size=config['hidden_size'],\n            n_layers=config['n_layers'],\n            dropout=config['dropout'],\n\n        )\n        self.organism_embedding = OrganismEmbedding(num_orgs=len(ORGANISMS), e_dim=config['hidden_size'])\n        self.classifier = Classifier(num_class=len(SP_LABELS), d_model=config['hidden_size'] * 2)\n\n    def forward(self, x, org):\n        x = self.input_embedding(x)\n        x, h_n, c_n = self.encoder(x)\n        x = x[:, -1, :]\n        org = self.organism_embedding(org)\n        inp = torch.cat((x, org), dim=1)\n        out = self.classifier(inp)\n        return out\n\n\n\"\"\" Stacked Bi-LTSM \"\"\"\n\n\nclass StackedBiLSTMClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.input_embedding = InputEmbedding(\n            vocab_size=config['vocab_size'],\n            d_model=config['d_model']\n        )\n        self.stacked_encoder = StackedBiLSTMEncoder(\n            embedding_dim=config['d_model'],\n            hidden_size=config['hidden_size'],\n            n_layers=config['n_layers'],\n            dropout=config['dropout'],\n\n        )\n        self.classifier = Classifier(num_class=len(SP_LABELS), d_model=config['hidden_size'] * 2)\n\n    def forward(self, x):\n        x = self.input_embedding(x)\n        x, h_n, c_n = self.stacked_encoder(x)\n        x = self.classifier(x[:, -1, :])\n        return x\n\n\nclass StackedBiLSTMOrganismClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.input_embedding = InputEmbedding(\n            vocab_size=config['vocab_size'],\n            d_model=config['d_model']\n        )\n        self.stacked_encoder = StackedBiLSTMEncoder(\n            embedding_dim=config['d_model'],\n            hidden_size=config['hidden_size'],\n            n_layers=config['n_layers'],\n            dropout=config['dropout'],\n\n        )\n        self.organism_embedding = OrganismEmbedding(num_orgs=len(ORGANISMS), e_dim=config['hidden_size'] * 2)\n        self.classifier = Classifier(num_class=len(SP_LABELS), d_model=config['hidden_size'] * 4)\n\n    def forward(self, x, org):\n        x = self.input_embedding(x)\n        x, h_n, c_n = self.stacked_encoder(x)\n        x = x[:, -1, :]\n        org = self.organism_embedding(org)\n        inp = torch.cat((x, org), dim=1)\n        out = self.classifier(inp)\n        return out\n\n\n\"\"\" CNN+Transformer \"\"\"\n\n\nclass CNNTransformerClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.input_embedding = InputEmbedding(\n            vocab_size=config['vocab_size'],\n            d_model=config['d_model']\n        )\n        self.conv_encoder = ConvolutionalEncoder(\n            embedding_dim=config['d_model'],\n            kernel_size=config['kernel_size']\n        )\n        self.positional_encoding = PositionalEncoding(\n            d_model=config['d_model'],\n            dropout=config['dropout'],\n            max_len=config['max_len']\n        )\n        self.trans_encoder = TransformerEncoder(\n            d_model=config['d_model'],\n            nhead=config['nhead'],\n            num_layers=config['num_layers']\n        )\n        self.classifier = Classifier(\n            d_model=config['d_model'],\n            num_class=len(SP_LABELS)\n        )\n\n    def forward(self, x):\n        x = self.input_embedding(x)\n        x = torch.transpose(x, 1, 2)\n        x = self.conv_encoder(x)\n        x = torch.transpose(x, 1, 2)\n        x = self.positional_encoding(x)\n        x = self.trans_encoder(x)\n        x = self.classifier(x)\n        return x\n\n\nclass CNNTransformerOrganismClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.input_embedding = InputEmbedding(\n            vocab_size=config['vocab_size'],\n            d_model=config['d_model']\n        )\n        self.conv_encoder = ConvolutionalEncoder(\n            embedding_dim=config['d_model'],\n            kernel_size=config['kernel_size']\n        )\n        self.positional_encoding = PositionalEncoding(\n            d_model=config['d_model'],\n            dropout=config['dropout'],\n            max_len=config['max_len']\n        )\n        self.trans_encoder = TransformerEncoder(\n            d_model=config['d_model'],\n            nhead=config['nhead'],\n            num_layers=config['num_layers']\n        )\n        self.classifier = Classifier(\n            d_model=config['d_model'] * 2,\n            num_class=len(SP_LABELS)\n        )\n        self.organism_embedding = OrganismEmbedding(\n            num_orgs=len(ORGANISMS),\n            e_dim=config['d_model']\n        )\n\n    def forward(self, x, org):\n        x = self.input_embedding(x)\n        x = torch.transpose(x, 1, 2)\n        x = self.conv_encoder(x)\n        x = torch.transpose(x, 1, 2)\n        x = self.positional_encoding(x)\n        x = self.trans_encoder(x)\n        org = self.organism_embedding(org)\n        inp = torch.cat((x, org), dim=1)\n        out = self.classifier(inp)\n        return out\n\n\n\"\"\" ProtBERT \"\"\"\n\n\nclass ProtBertClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bert = BertModel(config=config)\n        # self.bert_encoder = get_peft_model(encoder, peft_config)\n        if FREEZE_PRETRAINED:\n            self.freeze_pretrained_layer()\n        self.classifier = Classifier(num_class=len(SP_LABELS), d_model=config.hidden_size)\n\n    def forward(self, x):\n        x = self.bert(x)\n        x = x.last_hidden_state[:, 0, :]\n        x = self.classifier(x)\n        return x\n\n    def freeze_pretrained_layer(self):\n        for param in self.bert.parameters():\n            param.requires_grad = False\n\n\nclass ProtBertOrganismClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.bert = BertModel(config=config)\n        if FREEZE_PRETRAINED and MODEL_TYPE == \"bert_pretrained\":\n            self.freeze_pretrained_layer()\n        self.classifier = Classifier(num_class=len(SP_LABELS), d_model=config.hidden_size * 2)\n        self.organism_embedding = OrganismEmbedding(num_orgs=len(ORGANISMS), e_dim=config.hidden_size)\n\n    def forward(self, x, org):\n        x = self.bert(x)\n        x = x.last_hidden_state[:, 0, :]\n        org = self.organism_embedding(org)\n        inp = torch.cat((x, org), dim=1)\n        out = self.classifier(inp)\n        return out\n\n    def freeze_pretrained_layer(self):\n        for param in self.bert.parameters():\n            param.requires_grad = False\n\n\"\"\" GRAPH CONV \"\"\"\nclass GraphConvClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.graphconv_encoder = GraphConvEncoder(\n            d_model=config['d_model'],\n            dropout=config['dropout'],\n            use_relu_act=config['use_relu_act'],\n            d_hidden=config['d_hidden'],\n            use_special_tokens=False\n        )\n        self.classifier = Classifier(\n            d_model=config['d_model'] * 2,\n            num_class=len(params.SP_LABELS)\n        )\n\n    def forward(self, x):\n        x = self.graphconv_encoder(x, x.ndata['n_feat'])\n        x = torch.mean(x, dim=1)\n        x = self.classifier(x)\n        return x\n\n\nclass GraphConvOrganismClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.graphconv_encoder = GraphConvEncoder(\n            d_model=config['d_model'],\n            dropout=config['dropout'],\n            use_relu_act=config['use_relu_act'],\n            d_hidden=config['d_hidden']\n        )\n        self.organism_embedding = OrganismEmbedding(\n            num_orgs=len(ORGANISMS),\n            e_dim=config['d_model']\n        )\n        self.classifier = Classifier(\n            d_model=config['d_model'] * 2,\n            num_class=len(SP_LABELS)\n        )\n\n    def forward(self, x, org):\n        x = self.graphconv_encoder(x, x.ndata['n_feat'])\n        x = torch.mean(x, dim=1)\n        org = self.organism_embedding(org)\n        inp = torch.cat((x, org), dim=1)\n        out = self.classifier(inp)\n        return out\n\n\n\"\"\" GRAPH CONV TRANS \"\"\"\n\n\nclass GraphConvTransformerClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.graphconv_encoder = GraphConvEncoder(\n            d_model=config['d_model'],\n            dropout=config['dropout'],\n            use_relu_act=config['use_relu_act'],\n            d_hidden=config['d_hidden']\n        )\n        self.transformer_encoder = TransformerEncoder(\n            d_model=config['d_model'],\n            nhead=config['nhead'],\n            num_layers=config['num_layers']\n        )\n        self.classifier = Classifier(\n            d_model=config['d_model'],\n            num_class=len(SP_LABELS)\n        )\n\n    def forward(self, x, mask=None):\n        x, mask = self.graphconv_encoder(x, x.ndata['n_feat'])\n        x = self.transformer_encoder(x, mask)\n        x = self.classifier(x)\n        return x\n\n\nclass GraphConvTransformerOrganismClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.graphconv_encoder = GraphConvEncoder(\n            d_model=config['d_model'],\n            dropout=config['dropout'],\n            use_relu_act=config['use_relu_act'],\n            d_hidden=config['d_hidden']\n        )\n        self.transformer_encoder = TransformerEncoder(\n            d_model=config['d_model'],\n            nhead=config['nhead'],\n            num_layers=config['num_layers']\n        )\n        self.organism_embedding = OrganismEmbedding(\n            num_orgs=len(ORGANISMS),\n            e_dim=config['d_model']\n        )\n        self.classifier = Classifier(\n            d_model=config['d_model'] * 2,\n            num_class=len(SP_LABELS)\n        )\n\n    def forward(self, x, org, mask=None):\n        x, mask = self.graphconv_encoder(x, x.ndata['n_feat'])\n        x = self.transformer_encoder(x, mask)\n        org = self.organism_embedding(org)\n        inp = torch.cat((x, org), dim=1)\n        out = self.classifier(inp)\n        return out\n\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# import all model utils in model_utils.py\nfrom transformers import BertModel\n\n# Define types of model\nTRANSFORMER = 'transformer'\nCNN = 'cnn'\nLSTM = 'lstm'\nSTACKED_BILSTM = 'st_bilstm'\nBERT = 'bert'\nBERT_PRETRAINED = 'bert_pretrained'\nCNN_TRANSFORMER = 'cnn_trans'\nGCONV = 'gconv'\nGCONV_TRANSFORMER = 'gconv_trans'\n\n\ndef load_model(model_type, data_type, conf_type, use_organism=False):\n    config = load_config(model_type, data_type, conf_type)\n    if use_organism:\n        if model_type == TRANSFORMER:\n            return TransformerOrganismClassifier(config)\n        elif model_type == CNN:\n            return ConvolutionalOrganismClassifier(config)\n        elif model_type == STACKED_BILSTM:\n            return StackedBiLSTMOrganismClassifier(config)\n        elif model_type == BERT or model_type == BERT_PRETRAINED:\n            return ProtBertOrganismClassifier(config)\n        elif model_type == LSTM:\n            return LSTMOrganismClassifier(config)\n        elif model_type == CNN_TRANSFORMER:\n            return CNNTransformerOrganismClassifier(config)\n        elif model_type == GCONV and data_type == 'graph':\n            return GraphConvOrganismClassifier(config)\n        elif model_type == GCONV_TRANSFORMER and data_type == 'graph':\n            return GraphConvTransformerOrganismClassifier(config)\n        else:\n            return ValueError(\"Unknown model_type type\")\n    else:\n        if model_type == TRANSFORMER:\n            return TransformerClassifier(config)\n        elif model_type == CNN:\n            return ConvolutionalClassifier(config)\n        elif model_type == STACKED_BILSTM:\n            return StackedBiLSTMClassifier(config)\n        elif model_type == BERT or model_type == BERT_PRETRAINED:\n            return ProtBertClassifier(config)\n        elif model_type == LSTM:\n            return LSTMClassifier(config)\n        elif model_type == CNN_TRANSFORMER:\n            return CNNTransformerClassifier(config)\n        elif model_type == GCONV and data_type == 'graph':\n            return GraphConvClassifier(config)\n        elif model_type == GCONV_TRANSFORMER and data_type == 'graph':\n            return GraphConvTransformerClassifier(config)\n        else:\n            return ValueError(\"Unknown model_type type\")\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Directory _data_",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import dgl\n# code SPDataset (extends torch.utils.data.Dataset)\n\n# from transformers import BertTokenizer\n# from tokenizers import Tokenizer\n\n\nfrom typing import Optional\n\nfrom torch.utils.data import Dataset\n\n\nclass SPDataset(Dataset):\n    def __init__(self, json_paths: Optional[list[str]], data_type: str):\n        self.data_type = data_type\n        if json_paths is None or isinstance(json_paths, str):\n            raise ValueError('provide path to dataset in list of str')\n        df = pd.DataFrame(self._read_jsons(json_paths))\n        self.length = len(df)\n        self.labels = df['label'].tolist()\n        self.organisms = df['kingdom'].tolist()\n        if data_type == 'graph':\n            self.from_list = df['from_list'].tolist()\n            self.to_list = df['to_list'].tolist()\n            self.adj_matrix = df['adj_matrix'].tolist()\n        else:\n            self.smiles = df['smiles'].tolist()\n            self.aa_seq = df['aa_seq'].tolist()\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        organism = torch.tensor(ORGANISMS[self.organisms[index]])\n        label = torch.zeros(len(SP_LABELS), dtype=torch.int64)\n        label[SP_LABELS[self.labels[index]]] = 1\n        if self.data_type == 'graph':\n            graph = dgl.graph((self.from_list[index], self.to_list[index]))\n            graph = dgl.add_self_loop(graph)\n            graph.ndata['n_feat'] = torch.tensor(self.adj_matrix[index], dtype=torch.float)\n            return graph, label.clone().detach(), organism.clone().detach()\n        else:\n            seq = self.aa_seq[index] if self.data_type == 'aa' else self.smiles[index]\n            return seq, label.clone().detach(), organism.clone().detach()  # return (list[int], list[int], int)\n\n    @staticmethod\n    def _read_jsons(json_paths: list[str]):\n        data = []\n        for path in json_paths:\n            with open(path, 'r') as f:\n                data.extend(json.load(f))\n        return data\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### DataLoader",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from typing import List\nfrom torch.utils.data import Subset, Sampler, DataLoader, RandomSampler\n\n\nclass SPDataLoader(DataLoader):\n    def __init__(\n            self,\n            dataset,\n            shuffle=False,\n            use_workers_init_fn=False,\n            use_sp_sampler=False,\n            use_graph_collate_fn=False,\n            current_epoch=0,\n            batch_size=1,\n            num_workers=0,\n            pin_memory=False\n    ):\n        self.dataset = dataset\n        self.current_epoch = current_epoch\n        self.batch_size = batch_size\n        persistent_workers = False\n        if num_workers > 0:\n            persistent_workers = True\n        worker_init_fn = None\n        if use_workers_init_fn:\n            worker_init_fn = self.worker_init_fn\n        collate_fn = None\n        if use_graph_collate_fn:\n            collate_fn = SPDataLoader.graph_collate_fn\n        if shuffle and use_sp_sampler:\n            # warnings.warn(\"Do not set `shuffle` while using `use_sp_sampler`. Automatically set `shuffle=True`.\")\n            sp_sampler = SPBatchRandomSampler(dataset, batch_size, current_epoch, shuffle=True)\n            super().__init__(\n                dataset=dataset,\n                batch_sampler=sp_sampler,\n                num_workers=num_workers,\n                persistent_workers=persistent_workers,\n                worker_init_fn=worker_init_fn,\n                collate_fn=collate_fn,\n                pin_memory=pin_memory\n            )\n        else:\n            super().__init__(\n                dataset=dataset,\n                shuffle=shuffle,\n                batch_size=batch_size,\n                num_workers=num_workers,\n                persistent_workers=persistent_workers,\n                worker_init_fn=worker_init_fn,\n                collate_fn=collate_fn,\n                pin_memory=pin_memory\n            )\n\n    def worker_init_fn(self, worker_id):\n        seed = worker_id + self.current_epoch\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n\n    @staticmethod\n    def graph_collate_fn(batch):\n        graphs, lbs, organisms = map(list, zip(*batch))\n        g_feats = dgl.batch(graphs)\n        lbs = torch.stack(lbs)\n        organisms = torch.stack(organisms)\n        return g_feats, lbs, organisms\n\n\nclass SPBatchRandomSampler(Sampler[List[int]]):\n    def __init__(self, dataset: Dataset, batch_size: int, current_epoch: int, valid_indices=None, shuffle=False,\n                 replacement: bool = False, num_samples: Optional[int] = None, generator=None, drop_last=False):\n        super(Sampler, self).__init__()\n        if num_samples is None:\n            num_samples = len(dataset)\n        self.num_samples = num_samples\n        self.dataset = dataset  # dataset must implement __len__ method\n        if valid_indices is None:\n            valid_indices = range(len(dataset))\n        self.valid_indices = valid_indices\n        data_source = Subset(dataset, valid_indices)\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n        if shuffle and generator is None:\n            torch.manual_seed(current_epoch)\n            torch.cuda.manual_seed(current_epoch)\n        self.standard_sampler = RandomSampler(data_source=data_source, replacement=replacement,\n                                              num_samples=num_samples, generator=generator)\n\n    def __iter__(self):\n        batch = []\n        for idx in self.standard_sampler:\n            batch.append(idx)\n            if len(batch) == self.batch_size:\n                yield batch\n                batch = []\n        if len(batch) > 0 and not self.drop_last:\n            yield batch\n\n    def __len__(self):\n        return math.ceil(self.num_samples / self.batch_size)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Directory _tokenizer_",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import GPT2TokenizerFast, BertTokenizer\n\n\n# import utils from tokenizer_utils.py\ndef load_tokenizer(model_type, data_type):\n    if data_type in ['aa', 'smiles']:\n        tokenizer_path = str(Path(INPUT_DIR, 'sppredictor-tokenizer', f'tokenizer_{data_type}.json'))\n        tokenizer = GPT2TokenizerFast(tokenizer_file=tokenizer_path)\n        if tokenizer.pad_token is None:\n            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n        if model_type == 'bert_pretrained':\n            tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\")\n        return tokenizer\n    else:\n        return None",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Directory _lightning_module_",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from typing import Dict\n# code Lightning Data Module\nfrom typing import Optional\n\nimport lightning as L\nfrom lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n\n\n# from data.sp_dataset import SPDataset\n\n\nclass SPDataModule(L.LightningDataModule):\n    def __init__(\n            self,\n            data_type: str,\n            batch_size: int = 8,\n            num_workers: int = 1,\n            use_prepare_data: bool = False,\n            use_split_dataset: bool = False\n    ):\n        super().__init__()\n        self.current_training_epoch = 0\n        self.save_hyperparameters()\n\n        self.test_set = None\n        self.val_set = None\n        self.train_set = None\n\n        self.data_type = data_type\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.use_prepare_data = use_prepare_data\n        self.use_split_dataset = use_split_dataset\n        self.persistent_workers = False\n        if num_workers > 0:\n            self.persistent_workers = True\n        self.use_graph_collate_fn = False\n        if self.data_type == \"graph\":\n            self.use_graph_collate_fn = True\n\n    # def prepare_data(self) -> None:\n    #     if self.use_prepare_data and self.data_type == 'graph':\n    #         dut.extract_3d_dataset_by_partition()\n    #     elif self.use_prepare_data and self.data_type != 'graph':\n    #         dut.extract_raw_dataset_by_partition(raw_path=ut.abspath(params.TRAIN_PATH))\n    #         dut.extract_raw_dataset_by_partition(raw_path=ut.abspath(params.BENCHMARK_PATH), benchmark=True)\n\n    def state_dict(self) -> Dict[str, Any]:\n        state_dict = {\n            'current_training_epoch': self.trainer.current_epoch\n        }\n        return state_dict\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n        self.current_training_epoch = state_dict['current_training_epoch']\n\n    def setup(self, stage: Optional[str] = None) -> None:\n        if stage == \"fit\" or stage is None:\n            train_paths = [str(Path(INPUT_DIR, 'sppredictor-dataset', 'train_set_graph_partition_0.json')),\n                           str(Path(INPUT_DIR, 'sppredictor-dataset', 'train_set_graph_partition_1.json'))]\n            val_paths = [str(Path(INPUT_DIR, 'sppredictor-dataset', 'test_set_graph_partition_0.json')),\n                         str(Path(INPUT_DIR, 'sppredictor-dataset', 'test_set_graph_partition_0.json'))]\n            self.train_set = SPDataset(json_paths=train_paths, data_type=self.data_type)\n            self.val_set = SPDataset(json_paths=val_paths, data_type=self.data_type)\n        elif stage == \"test\":\n            test_paths = [str(Path(INPUT_DIR, 'sppredictor-dataset', 'train_set_graph_partition_2.json')),\n                          str(Path(INPUT_DIR, 'sppredictor-dataset', 'test_set_graph_partition_2.json'))]\n            self.test_set = SPDataset(json_paths=test_paths, data_type=self.data_type)\n\n    def train_dataloader(self) -> TRAIN_DATALOADERS:\n        return SPDataLoader(self.train_set, current_epoch=self.trainer.current_epoch, shuffle=True, use_sp_sampler=True,\n                            use_workers_init_fn=False, use_graph_collate_fn=self.use_graph_collate_fn,\n                            batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=True)\n\n    def val_dataloader(self) -> EVAL_DATALOADERS:\n        return SPDataLoader(self.val_set, current_epoch=self.trainer.current_epoch, shuffle=False, use_sp_sampler=False,\n                            use_workers_init_fn=False, use_graph_collate_fn=self.use_graph_collate_fn,\n                            batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=True)\n\n    def test_dataloader(self) -> EVAL_DATALOADERS:\n        return SPDataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, use_workers_init_fn=False,\n                            use_sp_sampler=False, use_graph_collate_fn=self.use_graph_collate_fn)\n\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nfrom sklearn.metrics import classification_report\nimport os.path\nfrom typing import Any, Dict\n\nimport lightning as L\nimport pandas as pd\nfrom torch import optim, Tensor\nfrom torch.nn import CrossEntropyLoss, Softmax\nfrom torch.optim import Optimizer\nfrom torchmetrics import F1Score, AveragePrecision, Recall\n\n\n# from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, matthews_corrcoef\n# from torcheval.metrics.functional import multiclass_auroc, multiclass_auprc, multiclass_f1_score\n\n\nclass SPModule(L.LightningModule):\n    def __init__(\n            self,\n            model_type: str,\n            data_type: str,\n            conf_type: str = 'default',\n            use_organism: bool = False,\n            batch_size: int = 8,\n            lr: float = 1e-7\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # Module params\n        self.model_type = model_type\n        self.data_type = data_type\n        self.conf_type = conf_type\n        self.use_organism = use_organism\n        self.batch_size = batch_size\n        self.lr = lr\n        if model_type == 'bert' or model_type == 'bert_pretrained':\n            self.lr = 1e-5  # according to TSignal, the learning rate for BERT model is fixed to 1e-5\n        self.checkpoint_name = ''\n\n#         loss_weight = torch.tensor([0.1, 0.3, 0.5, 0.5, 1, 1], dtype=torch.float)\n        loss_weight = torch.tensor([0.15, 1, 1, 1, 1, 1], dtype=torch.float)\n\n        self.loss_fn = CrossEntropyLoss(weight=loss_weight)\n        # self.fabric = Fabric()\n\n        # Load config (Remove if unnecessary)\n        # self.config = cut.load_config()\n\n        # Tokenizer\n        self.tokenizer = load_tokenizer(model_type=model_type, data_type=data_type)\n\n        # Load models\n        self.model = load_model(\n            model_type=model_type,\n            data_type=data_type,\n            conf_type=conf_type,\n            use_organism=use_organism\n        )\n\n        # Load metrics\n        self.f1 = F1Score(task='multiclass', num_classes=len(SP_LABELS), average=None)\n        self.recall = Recall(task=\"multiclass\", num_classes=len(SP_LABELS), average=None)\n        self.mcc = MCC(task='multiclass', num_classes=len(SP_LABELS), average=None)\n        self.average_precision = AveragePrecision(task='multiclass', num_classes=len(SP_LABELS), average=None)\n        # self.metrics = MulticlassMetrics(num_classes=len(params.SP_LABELS), average=None, device=self.device)\n\n        # Outputs from training process\n        self.validation_outputs_lb = []\n        self.validation_outputs_pred = []\n        self.best_val_loss = 1e6\n\n        self.test_outputs_lb_total = []\n        self.test_outputs_pred_total = []\n        self.test_outputs_lb_organism = [[], [], [], [], [], []]\n        self.test_outputs_pred_organism = [[], [], [], [], [], []]\n\n    def forward(self, x):\n        return self.model(x)\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=0.1)\n        return optimizer\n\n    def optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: Optimizer):\n        optimizer.zero_grad(set_to_none=True)\n\n    def backward(self, loss: Tensor, *args: Any, **kwargs: Any):\n        loss.backward()\n\n    def tokenize_input(self, x):\n        # max_length = 0\n        if self.model_type == 'bert' or self.model_type == 'bert_pretrained':\n            max_length = self.model.config.max_position_embeddings\n        else:\n            max_length = self.model.config['max_len']\n        encoded = self.tokenizer.batch_encode_plus(\n            x,\n            max_length=max_length,\n            truncation=True,\n            padding='max_length'\n        )\n        # print(len(encoded['input_ids'][0]))\n        return torch.tensor(encoded['input_ids'], dtype=torch.int64, device=self.device)\n\n    def base_step(self, batch, batch_idx):\n        x, lb, organism = batch\n        if self.tokenizer is not None:\n            x = self.tokenize_input(x)\n        # pred = None  # uncomment this line in case got error do not have variable `pred` defined\n        if self.use_organism:\n            pred = self.model(x, organism)\n        else:\n            pred = self.model(x)\n        loss = self.loss_fn(pred.float(), lb.float())\n        return x, lb, pred, loss, organism\n\n    def training_step(self, batch, batch_idx):\n        _, _, pred, loss, _ = self.base_step(batch, batch_idx)\n        self.log('train_loss', loss, on_epoch=True, prog_bar=True, batch_size=self.batch_size)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        _, lb, pred, loss, _ = self.base_step(batch, batch_idx)\n        self.validation_outputs_pred.extend(pred.tolist())\n        self.validation_outputs_lb.extend(lb.tolist())\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, batch_size=self.batch_size)\n        # return loss\n\n    def on_validation_epoch_end(self):\n        all_pred = torch.tensor(self.validation_outputs_pred, device=self.device)\n        all_lb = torch.tensor(self.validation_outputs_lb, device=self.device)\n\n        val_loss = self.loss_fn(all_pred.float(), all_lb.float())\n        if val_loss < self.best_val_loss:\n            self.best_val_loss = val_loss\n\n        all_lb = torch.argmax(all_lb, dim=1)\n\n        self.f1.update(all_pred, all_lb)\n        self.recall.update(all_pred, all_lb)\n        self.mcc.update(all_pred, all_lb)\n        self.average_precision.update(all_pred, all_lb)\n\n        print(\n            f\"\\nMetrics on validation set: \"\n            f\"best_val_loss: {self.best_val_loss}, \"\n            f\"f1: {self.f1.compute()}, \"\n            f\"recall: {self.recall.compute()}, \"\n            f\"mcc: {self.mcc.compute()}, \"\n            f\"average_precision: {self.average_precision.compute()} \\n\"\n        )\n\n        self.validation_outputs_lb.clear()\n        self.validation_outputs_pred.clear()\n        self.f1.reset()\n        self.recall.reset()\n        self.mcc.reset()\n        self.average_precision.reset()\n\n    def test_step(self, batch, batch_idx):\n        _, lb, pred, loss, organism = self.base_step(batch, batch_idx)\n\n        pred = pred.clone().detach()\n\n        # Update outputs for calculate metrics on each class (for total)\n        self.test_outputs_pred_total.extend(pred.tolist())\n        self.test_outputs_lb_total.extend(lb.tolist())\n\n        # Update outputs for calculate metrics on each class (for each organism)\n        for i, o in enumerate(organism):\n            self.test_outputs_pred_organism[o].append(pred[i].tolist())\n            self.test_outputs_lb_organism[o].append(lb[i].tolist())\n\n    def on_test_end(self) -> None:\n        # TODO: Tạo một metrics dict để lưu các giá trị này lại và print (xem xét tạo một func như class_report của sklearn\n\n        softmax = Softmax()\n\n        # Apply argmax on these outputs (only for label) and evaluate the metric results\n        total_pred = torch.tensor(self.test_outputs_pred_total, device=self.device)\n        total_lb = torch.tensor(self.test_outputs_lb_total, device=self.device)\n        print(classification_report(torch.argmax(total_pred, dim=1).tolist(), torch.argmax(total_lb, dim=1).tolist(),\n                                    zero_division=0))\n\n        # Calculate metrics on each class (for both on total and on organisms)\n        total_index = len(ORGANISMS)\n        f1_test = [[], [], [], [], [], [], []]\n        recall_test = [[], [], [], [], [], [], []]\n        mcc_test = [[], [], [], [], [], [], []]\n        average_precision_test = [[], [], [], [], [], [], []]\n        for k, o in ORGANISMS.items():\n            all_pred = softmax(torch.tensor(self.test_outputs_pred_organism[o], device=self.device))\n            all_lb = torch.tensor(self.test_outputs_lb_organism[o], device=self.device)\n            all_lb = torch.argmax(all_lb, dim=1)\n\n            # Print the statistic (the following function has ERROR about syntax)\n            self._save_results_to_txt(all_pred.clone().detach().cpu(), all_lb.clone().detach().cpu(), organism=k)\n\n            f1_test[o] = (self.f1(all_pred, all_lb) * 100).tolist()\n            recall_test[o] = (self.recall(all_pred, all_lb) * 100).tolist()\n            mcc_test[o] = (self.mcc(all_pred, all_lb) * 100).tolist()\n            average_precision_test[o] = (self.average_precision(all_pred, all_lb) * 100).tolist()\n\n            print(\n                f'\\nMetrics on test set of {k}: '\n                f'f1: {f1_test[o]}, '\n                f'recall: {recall_test[o]}, '\n                f'mcc: {mcc_test[o]}, '\n                f'average_precision: {average_precision_test[o]} \\n'\n            )\n\n            self.f1.reset()\n            self.recall.reset()\n            self.mcc.reset()\n            self.average_precision.reset()\n\n        all_pred = total_pred\n        all_lb = total_lb\n        all_lb = torch.argmax(all_lb, dim=1)\n\n        self.f1.update(all_pred, all_lb)\n        self.recall.update(all_pred, all_lb)\n        self.mcc.update(all_pred, all_lb)\n        self.average_precision.update(all_pred, all_lb)\n        f1_test[total_index] = (self.f1.compute() * 100).tolist()\n        recall_test[total_index] = (self.recall.compute() * 100).tolist()\n        mcc_test[total_index] = (self.mcc.compute() * 100).tolist()\n        average_precision_test[total_index] = (self.average_precision.compute() * 100).tolist()\n\n        print(\n            f'\\nMetrics on test set of TOTAL: '\n            f'f1: {f1_test[total_index]}, '\n            f'recall: {recall_test[total_index]}, '\n            f'mcc: {mcc_test[total_index]}, '\n            f'average_precision: {average_precision_test[total_index]} \\n'\n        )\n\n        self.f1.reset()\n        self.recall.reset()\n        self.mcc.reset()\n        self.average_precision.reset()\n\n        metric_dict = {\n            \"f1_score\": f1_test,\n            \"recall\": recall_test,\n            \"mcc\": mcc_test,\n            \"average_precision\": average_precision_test,\n        }\n\n        print(metric_dict)\n\n        self._save_metrics_to_csv(metric_dict)\n\n    def on_load_checkpoint(self, checkpoint: Dict[str, Any]) -> None:\n        self.checkpoint_name = CHECKPOINT.split('.')[0]\n\n    def _save_results_to_txt(self, test_prediction_results, test_true_results, organism):\n        if not os.path.exists(str(Path(f\"{WORKING_DIR}/out/results\"))):\n            os.makedirs(f\"{WORKING_DIR}/out/results\", exist_ok=True)\n\n        softmax = Softmax()\n        pred_path = f'{WORKING_DIR}/out/results/{organism}_test_prediction_by_{self.model_type}.txt'\n        true_path = f'{WORKING_DIR}/out/results/{organism}_test_true.txt'\n\n        np.savetxt(pred_path, softmax(test_prediction_results), fmt=\"%.4f\")\n        # np.savetxt(ut.abspath(true_path), test_true_results, fmt=\"%d\")\n        if not os.path.exists(true_path):\n            np.savetxt(true_path, test_true_results, fmt=\"%d\")\n\n    def _save_metrics_to_csv(self, metric_dict):\n        if not os.path.exists(str(Path(f\"{WORKING_DIR}/out/metrics\"))):\n            os.makedirs(f\"{WORKING_DIR}/out/metrics\", exist_ok=True)\n\n        for k, o in ORGANISMS.items():\n            metrics_organisms = {\n                \"f1_score\": metric_dict['f1_score'][o],\n                \"recall\": metric_dict['recall'][o],\n                \"mcc\": metric_dict['mcc'][o],\n                \"average_precision\": metric_dict['average_precision'][o],\n            }\n            df = pd.DataFrame.from_dict(metrics_organisms).transpose().round(2)\n            df.to_csv(str(Path(f'{WORKING_DIR}/out/metrics/{self.checkpoint_name}_test_{k}.csv')),\n                      header=list(SP_LABELS.keys()), index_label='metrics', na_rep=str(0.0))\n\n        total_index = len(ORGANISMS)\n        metrics_total = {\n            \"f1_score\": metric_dict['f1_score'][total_index],\n            \"recall\": metric_dict['recall'][total_index],\n            \"mcc\": metric_dict['mcc'][total_index],\n            \"average_precision\": metric_dict['average_precision'][total_index],\n        }\n        df = pd.DataFrame().from_dict(metrics_total).transpose().round(2)\n        df.to_csv(str(Path(f'{WORKING_DIR}/out/metrics/{self.checkpoint_name}_test_metrics_TOTAL.csv')),\n                  header=list(SP_LABELS.keys()), index_label='metrics', na_rep=str(0.0))\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Training and Validation\n\nimport lightning as L\nimport torch\n# from lightning.pytorch.loggers.tensorboard import TensorBoardLogger\nfrom lightning.pytorch.loggers import WandbLogger\nfrom pathlib import Path\nimport os\nimport wandb\n\n\ndef train():\n    torch.set_float32_matmul_precision('medium')\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    torch.cuda.empty_cache()\n\n    # CLI parsing arguments\n    log_dir = str(Path(WORKING_DIR, LOG_DIR))\n    logger = False\n    if USE_LOGGER:\n        # access wandb\n        wandb_api = '3c8685dbfce5b23f56fce47a675b7a3569dead2c'\n        wandb.login(key=wandb_api)\n        logger = WandbLogger(save_dir=log_dir, project='SPPredictor')\n        if USE_ORGANISM:\n            logger.experiment.name = f'{MODEL_TYPE}_{DATA_TYPE}_use_organism'\n        else:\n            logger.experiment.name = f'{MODEL_TYPE}_{DATA_TYPE}'\n        logger.experiment.config['batch_size'] = BATCH_SIZE\n\n    resume_ckpt = f'{MODEL_TYPE}-{DATA_TYPE}-{CONF_TYPE}-{int(USE_ORGANISM)}_epochs{EPOCHS}.ckpt'\n    checkpoint = str(Path(INPUT_DIR, 'checkpoints', resume_ckpt))\n    if not os.path.exists(checkpoint):\n        checkpoint = None\n\n    sp_module = SPModule(\n        model_type=MODEL_TYPE,\n        data_type=DATA_TYPE,\n        conf_type=CONF_TYPE,\n        use_organism=USE_ORGANISM,\n        batch_size=BATCH_SIZE,\n        lr=LEARNING_RATE,\n    )\n\n    sp_data_module = SPDataModule(\n        data_type=DATA_TYPE,\n        batch_size=BATCH_SIZE,\n        num_workers=NUM_WORKERS,\n    )\n\n    trainer = L.Trainer(\n        devices=DEVICES,\n        accelerator=ACCELERATOR,\n        max_epochs=EPOCHS,\n        logger=logger,\n        val_check_interval=1.0,\n        callbacks=[model_checkpoint],\n    )\n\n    trainer.fit(sp_module, datamodule=sp_data_module, ckpt_path=checkpoint)\n\n    if logger:\n        wandb.finish(quiet=True)\n\n\ndef test():\n    torch.set_float32_matmul_precision('medium')\n\n    # args = parse_arguments()\n    checkpoint = str(Path(f'{INPUT_DIR}/checkpoints/{CHECKPOINT}'))\n    if not os.path.exists(checkpoint):\n        raise FileNotFoundError(\"Path does not exist. Check checkpoint path again\")\n\n    sp_module = SPModule.load_from_checkpoint(checkpoint_path=checkpoint)\n    sp_data_module = SPDataModule.load_from_checkpoint(checkpoint_path=checkpoint)\n\n    trainer = L.Trainer(\n        devices=DEVICES,\n        accelerator=ACCELERATOR,\n        logger=False,\n        enable_checkpointing=False\n    )\n\n    trainer.test(sp_module, datamodule=sp_data_module)\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train()",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# test()",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
